{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d914e2",
   "metadata": {},
   "source": [
    "# （一）背景描述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020fc0cf",
   "metadata": {},
   "source": [
    "### 1、问题提出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b7926",
   "metadata": {},
   "source": [
    "在红酒的各项特征中，可以通过酒精浓度和红酒的颜色判断出红酒的年份，通过检测红酒中酚类化合物、黄酮类化合物成分判断红酒是否过期。因此我们可以通过对红酒各项成分含量的分析，来判断出红酒的品质，也就可以判断出红酒的所属类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba8cdf9",
   "metadata": {},
   "source": [
    "判别红酒分类主要有感官评定方法和常规理化指标检测方法，虽然感官评定是目前国内外鉴定葡萄酒品质的主要手段，但由于该方法主要是评定人员通过红酒颜色强度、气味、色调和滋味等感官指标来对红酒进行分类，因此该方法具有较大的主观性。为了减少感官评定方法可能会产生的红酒分类错误，本例子采用深度学习中的全连接神经网络结构对红酒常规理化指标进行模型训练，并以学得的模型预测红酒所属类别，以提高红酒分类效率。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e1e1620",
   "metadata": {},
   "source": [
    "| 列名 | 说明 | 属性类型 |\n",
    "| --- | --- | --- |\n",
    "| class | 三个类别：class0，class1，class2 | 离散 |\n",
    "| alcohol | 酒精浓度(%) | 连续 |\n",
    "| malic_acid | 苹果酸 | 连续 |\n",
    "| ash | 灰烬 | 连续 |\n",
    "| alcalinity_of_ash | 灰分的碱度 | 连续 |\n",
    "| magnesium | 镁(mg/L) | 连续 |\n",
    "| total_phenols | 总酚(mg/L) | 连续 |\n",
    "| flavanoids | 黄酮类化合物 | 连续 |\n",
    "| nonflavanoid_phenols | 非黄烷类酚类 | 连续 |\n",
    "| proanthocyanins | 原花色素 | 连续 |\n",
    "| color_intensity | 颜色强度 | 连续 |\n",
    "| hue | 色调 | 连续 |\n",
    "| od280/od315_of_diluted_wines | 稀释葡萄酒的OD280/OD315 | 连续 |\n",
    "| proline | 脯氨酸 | 连续 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a0aaf1a",
   "metadata": {},
   "source": [
    "### 2、研究目的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8ebcd39",
   "metadata": {},
   "source": [
    "基于红酒常规理化指标数据，训练学得一个可以预测红酒分类的全连接神经网络模型，并研究使用该模型进行红酒分类的准确率。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8043db3",
   "metadata": {},
   "source": [
    "# （二）数据处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec51bd7e",
   "metadata": {},
   "source": [
    "### 1、数据来源"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09615cf7",
   "metadata": {},
   "source": [
    "Wine葡萄酒数据集是来自UCI上面的公开数据集，该数据是对意大利在不同地点所生产的三种葡萄酒进行化学分析的结果。 由于sklearn自带的红酒数据集（wine）与UCI上面的公开数据集的数据内容一样，所以本实验直接通过sklearn.datasets导入红酒数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine  #红酒数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dcfec01",
   "metadata": {},
   "source": [
    "### 2、数据描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33224585",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "wine = load_wine()\n",
    "x = wine.data  #特征数据\n",
    "y = wine.target  #标签数据\n",
    "features_name = wine.feature_names\n",
    "target_name = wine.target_names\n",
    "print('特征名称：',features_name)\n",
    "print('标签类别：',target_name)\n",
    "\n",
    "df1 = pd.DataFrame(data=x, columns=features_name)  #将特征数据转化为Dataframe\n",
    "df2 = pd.DataFrame(data=y, columns=['class'])  #将标签数据转化为Dataframe\n",
    "df = pd.concat([df1,df2],axis=1)  #合并特征数据和标签数据\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af00f979",
   "metadata": {},
   "source": [
    "可以看到在wine数据集中，共有178个样本，每个样本有13个特征（前13列数据），最后一列为红酒所属类别（其中第1类有59个样本，第2类有71个样本，第3类有48个样本）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9aad1fe",
   "metadata": {},
   "source": [
    "### 3、数据预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72c926ce",
   "metadata": {},
   "source": [
    "数据分割：将80%的数据集作为训练集，其余的20%为测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb2c526",
   "metadata": {},
   "source": [
    "归一化处理：通过调用sklearn库的标准化函数MinMaxScaler对数据进行归一化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c23121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x_train0)   # 使用了离差标准化方法进行归一化处理\n",
    "x_test = min_max_scaler.fit_transform(x_test0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e3fb46d",
   "metadata": {},
   "source": [
    "转化数据为Tensor类型：\n",
    "\n",
    "Tensor（张量）是PyTorch中重要的数据结构，是存储和变换数据的主要⼯具，并且Tensor和Numpy的多维数组⾮常类似，但Tensor可以使用GPU进行加速计算。因为本实验是基于PyTorch框架实现红酒分类，Tensor是深度学习中核心的数据结构，所以需要将数据转换为Tensor类型。\n",
    "\n",
    "接下来我们需要对训练和测试的特征标签转换为Tensor格式，默认的Tensor是数据类型是FloatTensor。\n",
    "- 由上面数据初探可以发现，红酒数据的特征的数据类型都是float64，所以我们将训练和测试的特征转为FloatTensor的32位浮点类型数据。\n",
    "- 由于标签为类别为int32，如果按默认格式转为浮点型数据下面运行会报错，所以我们将训练和测试的标签转为LongTensor的64位整型数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049ee0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "# 将数据转换为tensor类型\n",
    "x_train = torch.FloatTensor(x_train)  \n",
    "y_train = torch.LongTensor(y_train)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "print(y_test) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f021f3e",
   "metadata": {},
   "source": [
    "# （三）模型构建"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e62f62bf",
   "metadata": {},
   "source": [
    "通过创建一个类，定义全连接神经网络\n",
    "+  构建子模块：在自己建立的模型（继承torch.nn.Module）的__init__()方法\n",
    "+  拼接子模块：在模型的forward()方法中\n",
    "\n",
    "本实验使用的是两层的全连接神经网络模型，采用relu()函数作为隐藏层的激活函数，采用sigmoid()函数作为输出层的激活函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e892affa",
   "metadata": {},
   "source": [
    "### 1、设计全连接神经网络的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 建立\n",
    "class NetModel(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(NetModel, self).__init__()\n",
    "        self.hiddden = torch.nn.Linear(n_feature, n_hidden, bias = True)  # 定义隐层网络\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output, bias = True)  # 定义输出层网络\n",
    "        self.relu = nn.ReLU()   # 定义relu激活函数\n",
    "        #self.dropout = nn.Dropout(p=0.5)   # 定义dropout正则化，以概率0.5随机的将参数置0\n",
    "        self.sigmoid = nn.Sigmoid()   # 定义sigmoid激活函数\n",
    "        \n",
    "    # forward()是实现网络中不同层的连接关系（即前向传播）\n",
    "    def forward(self, x):\n",
    "        x = self.hiddden(x)\n",
    "        x = self.relu(x)  # 隐藏层激活函数采用relu()函数\n",
    "        #x = self.dropout(x)   # 如果使用了dropout正则化，这里需要删除注释符号\n",
    "        x = self.out(x)\n",
    "        x = self.sigmoid(x)   # 预测值\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36fc5e4e",
   "metadata": {},
   "source": [
    "# （四）训练模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0555249b",
   "metadata": {},
   "source": [
    "### 1、定义模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt   # 函数定义中存在绘制每次迭代损失值的图像，因此需要导入该模块\n",
    "import numpy as np\n",
    "#配置中文显示\n",
    "plt.rcParams['font.family'] = ['SimHei'] \n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "# 参数的解释\n",
    "def fit(net, loader, lr, epochs):\n",
    "    # 对于多分类一般使用交叉熵损失函数\n",
    "    loss_fun = torch.nn.CrossEntropyLoss()\n",
    "    # 使用Adam优化器\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    running_loss = []   # 用于保存每次迭代的loss值\n",
    "    for i in range(epochs):\n",
    "        print(\"---------第{}轮训练开始------------\".format((i + 1)))\n",
    "        train_loss = 0.0   # 用于汇总每批次计算出来的loss值\n",
    "        train_acc = 0.0   # 用于汇总每批次计算出来正确的预测数量\n",
    "        \n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            \n",
    "            output = net.forward(batch_x)   # 执行前向传播\n",
    "            train_acc += (output.max(1)[1] == batch_y).sum().item()\n",
    "            loss = loss_fun(output, batch_y)   # 计算损失值\n",
    "            \n",
    "            loss.backward()   # 反向传播\n",
    "            optimizer.step()   # 更新梯度\n",
    "            optimizer.zero_grad()   # 梯度清零\n",
    "            \n",
    "            train_loss += loss.item()            \n",
    "            # 每到10个批次时（因为当前样本量较少，当样本量较多时可以适当增大这个值）或到达最后一个批次时，输出一次当前loss值\n",
    "            if (step + 1) % 10 == 0 or step == (len(loader) - 1):\n",
    "                print('迭代次数：%d, batch数：%5d 当前loss值: %.3f' % (i + 1, np.ceil((step + 1)/10), train_loss))\n",
    "        \n",
    "        print('第%d次完整迭代的损失值：%.3f' % (i + 1, train_loss))\n",
    "        train_acc = train_acc/len(x_train) * 100\n",
    "        print('第%d次完整迭代的正确率：%.1f%%' % (i + 1, train_acc))\n",
    "        \n",
    "        running_loss.append(train_loss)\n",
    "    \n",
    "    # 绘制损失变化图\n",
    "    plt.plot(np.arange(epochs), running_loss, color = 'r', alpha = 0.3)\n",
    "    plt.xticks(np.arange(0, epochs, 5))\n",
    "    plt.title('训练集损失函数值随迭代次数的变化趋势')\n",
    "    plt.xlabel('epoches')\n",
    "    plt.ylabel('train_loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f9f1c4f",
   "metadata": {},
   "source": [
    "### 2、定义模型测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNN(net, x_test, y_test):\n",
    "    # 定义损失函数\n",
    "    loss_fun = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():   # 模型在测试集中不需要进行反向传播\n",
    "        test_acc = 0.0   #辅助计算测试集的准确率\n",
    "        output = net.forward(x_test)\n",
    "        test_acc += (output.max(1)[1] == y_test).sum().item()\n",
    "        loss = loss_fun(output, y_test)   # 计算损失值\n",
    "\n",
    "        print('测试集的损失值：%.3f' % (loss.item()))\n",
    "        print('测试集的正确率：%.1f%%' % (test_acc/len(x_test) * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97f08780",
   "metadata": {},
   "source": [
    "### 3、开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6899be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = len(x_train)   # 梯度下降法一组的样本数量，这里使用了训练集整个样本集，即为全批量梯度下降法\n",
    "loader = data.DataLoader(\n",
    "    dataset=data.TensorDataset(x_train, y_train),\n",
    "    batch_size=batch_size,   # 若更改batch_size为1到len(x_train)之间，即为mini-batch梯度下降法；若值为1，则为SGD梯度下降法\n",
    "    shuffle=True   # 在每次迭代训练时是否将数据洗牌 \n",
    ")\n",
    "\n",
    "net = NetModel(13, 20, 3)\n",
    "fit(net, loader=loader, lr=0.15, epochs=20)   # 学习率初始化为0.15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5912233c",
   "metadata": {},
   "source": [
    "### 4、测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "testNN(net, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8bd788b",
   "metadata": {},
   "source": [
    "# （五）模型评价"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d2ae014",
   "metadata": {},
   "source": [
    "### 1、学习率的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef84033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了防止输出内容较多，对训练模型的函数里使用了print函数的代码进行注释\n",
    "# 由于这一小节需要绘制不同alpha值下，成本函数值随迭代次数增加的变化趋势，因此我们还需要在训练模型函数的最后加入一行返回代码，返回不同迭代次数下的损失函数值\n",
    "\n",
    "# 改写训练模型的函数，加多一个参数methods，表示使用哪一种优化算法进行模型训练\n",
    "def fit_upd(net, loader, lr, epochs, methods='optAda'):   # 训练模型的函数默认使用Adam优化算法\n",
    "    # 对于多分类一般使用交叉熵损失函数\n",
    "    loss_fun = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    if methods =='optMomentum':\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=lr)   # 动量梯度优化算法\n",
    "    elif methods =='optRMSprop':\n",
    "        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)   # RMSprop优化算法\n",
    "    else :\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)   # Adam优化算法\n",
    "    \n",
    "    running_loss = []   # 用于保存每次迭代的loss值\n",
    "    for i in range(epochs):\n",
    "        train_loss = 0.0   # 用于汇总每批次计算出来的loss值\n",
    "        \n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            output = net.forward(batch_x)   # 执行前向传播\n",
    "            loss = loss_fun(output, batch_y)   # 计算损失值\n",
    "            loss.backward()   # 反向传播\n",
    "            optimizer.step()   # 更新梯度\n",
    "            optimizer.zero_grad()   # 梯度清零\n",
    "            train_loss += loss.item()            \n",
    "        \n",
    "        running_loss.append(train_loss)\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = np.arange(0.09, 1, 0.2)   # 取多个alpha值，并观察不同alpha值下成本函数值随迭代次数增加的变化趋势\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train_loss')\n",
    "plt.title('不同alpha值在此网络模型中的训练速度比较')\n",
    "\n",
    "for i in np.arange(len(LR)):\n",
    "    net=NetModel(13, 20, 3)   # 构建神经网络模型\n",
    "    loss = fit_upd(net, loader, lr=round(LR[i], 2), epochs=100)\n",
    "    ax.plot(loss, label=round(LR[i], 2))\n",
    "    plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c397aee",
   "metadata": {},
   "source": [
    "### 2、比较不同优化算法的收敛速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  在学习率的选择一节中，可得出学习率在0.1左右时，模型可以较快地进行收敛，因此此处设置alpha=0.1\n",
    "lr = 0.1\n",
    "optimizers = ['optAda', 'optMomentum', 'optRMSprop']   # 不同的优化算法\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('train_loss')\n",
    "plt.title('A comparation of the speed of different algrithoms')\n",
    "\n",
    "for i in optimizers:\n",
    "    net=NetModel(13, 20, 3)   # 构建神经网络模型\n",
    "    loss = fit_upd(net, loader, lr=lr, epochs=100, methods=i)\n",
    "    ax.plot(loss, label=i)\n",
    "    plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39b98368",
   "metadata": {},
   "source": [
    "### 3、Droupout正则化的使用"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6447639a",
   "metadata": {},
   "source": [
    "在模型构建类中已定义了droupout正则化，同学们可通过删除注释符号观察正则化后的模型效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b078357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S:\n",
    "    v=0\n",
    "    @property\n",
    "    def func(self):\n",
    "        self.var=S.v\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3507844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "this is decorated function\n",
      "func0=decorator(func0)\n",
      "this is formor function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'anything'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decorator(f):\n",
    "    def func(*args):\n",
    "        print(args)\n",
    "        print('this is decorated function')\n",
    "        print('func0=decorator(func0)')\n",
    "        f()\n",
    "        return 'anything'\n",
    "    return func\n",
    "\n",
    "@decorator\n",
    "def func0():\n",
    "    print('this is formor function')\n",
    "\n",
    "func0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d0881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867fcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbfc99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
